# 自分の理解用メモ

## 基本知識
- ブロードキャスト
  - 形状の異なるテンソル同士を計算できるようにする自動拡張機能
    - 例: [1, 2, 3] + [5] = [6, 7, 8]
      - [5]が、[5,5,5]に拡張している
- トークン数
  - 受け取る文章のトークンの数
  - 例: "I am happy today" -> 4トークン
- 埋め込み次元
  - 単語の意味をベクトルで表現する方法
  - ソースの例だと、埋め込み次元は768次元で表現される。
    - 今回のソースでは、ヘッド数が12なので64次元で分割される。
    - このとき、64次元という数値が元の埋め込み次元から意味のある形で分割できているかはわからない。なんならもとの埋め込み次元から情報の損失が起きているかもしれない
    - それも含めて複数のヘッドが、学習時にその分割された限られた範囲で特徴を捉え、重み(query,key,values)に変換することで、意味のある形で64次元に圧縮し、新しい形で情報をとらえている
- 順伝播
  - 入力データをネットワークに通して予測/出力を得る
- 誤差逆伝播
  - 損失(予測と正解の差) から各パラメータに対する勾配を計算
  - これより各パラメータをどう変えれば損失が減るか、がわかる
- ドロップアウト
  - 過学習を防ぐ正則化手法
  - 学習時にランダムにニューロンを無効化（0にする）し、残りを拡大する
  - 推論時は全ニューロンを使用
  - 特定のニューロンへの依存を防ぎ、汎化性能を向上させる
- LoRA(Low-Rank Adaptation)
  - 大規模なモデルの効率的なファインチューニング方法
  - 元の重み行列はそのままで、小さな行列を追加して学習する
    - 元のものに触らないことで、学習時に更新するパラメータを大幅に削減でき、さらに高い性能を維持できる

### PyTorch関連
- nn.Module
  - PyTorchのニューラルネットワークを作る際の基本クラス
  - 継承時は`super().__init__()`が必須
    - パラメータ管理、デバイス管理、モード切り替えなどがある
- nn.Embedding
  - ルックアップテーブル(参照表)を作成するPyTorchの層
    - 具体的には [50257(語彙) × 768(次元)] の行列など
    - このとき単語ID=1のものは1行目のベクトルが埋め込みとして参照される
- nn.Sequential
  - 複数の層を順番に実行するコンテナ
- nn.Parameter
  - 学習可能なパラメータを作成する
    - torch.ones()のままだと学習時に更新されないが、Parameterでくくることで学習可能に
- nn.Linear
  - 線形変換(全結合層) を行う層
  - y=xW^T + b
  - Linear層は自動的に最後の次元に対して行列乗算する
    - 例えば(2,4,768)@(768,768)だと、(2,4,768)となる
      - @は行列演算(内積)の演算子
      - 最後の次元は(2,4,768)の768
  - 学習可能なパラメータを持つ
- keepdim=True
  - 計算後も元のテンソルと同じ次元数を維持する
- dim=-1
  - 最後の次元を指す。最後の次元に対して計算したい場合などに利用
- view
  - tensorの形状(次元) を変更したい際に、データは変えずに形状だけ変える
    - Wのような大きなtensorを本来の複数のヘッドの状態に戻して、それぞれ独立して計算をし、多ヘッドで多彩な解釈ができるようになる
  - また副次的なメリットとして、GPUの特性上小さな行列の並列計算が得意なのでそういう観点でもメリットがある

# 全体の流れ
TODO:

# それぞれのコンポーネント
## GPTModel
- トークン埋め込み層
  - 受け取った単語IDの配列を、埋め込み次元の配列に変換する
- 位置埋め込み層
  - [1024(コンテキストの長さ) × 768(次元)]
  - 最初はランダムな数値が設定される学習可能なパラメータ
- Transformerブロック

## TransformerBlock


## FeedForward
- フィードフォワード
  - 各位置の特徴を独立に拡大/縮小変換し、さらに非線形変換を通してより表現力のある空間の探索を可能にする
  - 具体的には、まず高次元の空間に拡張(拡大変換)し、それをGELUで非線形変換、その後元の次元に戻す(縮小変換)
  - データから学習して汎化するモデルの能力を向上させるうえで、重要な役割を果たす
  - なぜ4倍? -> 過去の研究により、4倍にするのが経験的に良い結果を示している

## LayerNorm
- Layer Normalization, 層正規化
- コンセプトはNN層の出力を平均0,分散1になるよう調整すること
  - これにより、効果的な重みへの収束が早まり、訓練の一貫性と信頼性を確保する
- ただ標準化するだけでなく、学習可能なパラメータであるscaleとshiftを利用し、より柔軟に標準化を実施する 
  - 具体的には特定の次元(768のうちの)の影響が大きいと学習時に発覚した場合に、重要度の表現の仕方としてscaleの値を大きく設定するなど

## MultiHeadAttention

- ヘッドとは
  - 異なる注意の向け方を学習する機構
    - 例えば、文法の関係に注目したヘッドや、意味的な関係に注目したヘッド、修飾関係に注目したヘッドなど
- マルチヘッドとは
  - ヘッドが複数あること(マルチ)
    - 複数あることで上述したような様々な観点から注意を向けることで文章に対する理解がより深まる
  - 1つの注意機構のヘッドでは、1つの関係性しか捉えられないが、複雑な文章では同時に複数の関係性が重複している
  - 理論的には複数のヘッドを用いてそれぞれ独立して計算をするが、
  - 実装面でいうとヘッドのベクトルをすべて直列に結合し、大きなベクトルとしてまとめて計算を行っている
  - 理論をそのまま実装はせず、計算効率のためにうまく工夫されている
- 処理の流れ
  - TODO
