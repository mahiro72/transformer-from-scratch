# 自分の理解用メモ

## 基本知識
- ブロードキャスト
  - 形状の異なるテンソル同士を計算できるようにする自動拡張機能
    - 例: [1, 2, 3] + [5] = [6, 7, 8]
      - [5]が、[5,5,5]に拡張している
- トークン数
  - 受け取る文章のトークンの数
  - 例: "I am happy today" -> 4トークン
- 埋め込み次元
  - 単語の意味をベクトルで表現する方法
  - ソースの例だと、埋め込み次元は768次元で表現される。
    - 今回のソースでは、ヘッド数が12なので64次元で分割される。
    - このとき、64次元という数値が元の埋め込み次元から意味のある形で分割できているかはわからない。なんならもとの埋め込み次元から情報の損失が起きているかもしれない
    - それも含めて複数のヘッドが、学習時にその分割された限られた範囲で特徴を捉え、重み(query,key,values)に変換することで、意味のある形で64次元に圧縮し、新しい形で情報をとらえている
- 順伝播
  - 入力データをネットワークに通して予測/出力を得る
- 誤差逆伝播
  - 損失(予測と正解の差) から各パラメータに対する勾配を計算
  - これより各パラメータをどう変えれば損失が減るか、がわかる
- ドロップアウト
  - 過学習を防ぐ正則化手法
  - 学習時にランダムにニューロンを無効化（0にする）し、残りを拡大する
  - 推論時は全ニューロンを使用
  - 特定のニューロンへの依存を防ぎ、汎化性能を向上させる
- LoRA(Low-Rank Adaptation)
  - 大規模なモデルの効率的なファインチューニング方法
  - 元の重み行列はそのままで、小さな行列を追加して学習する
    - 元のものに触らないことで、学習時に更新するパラメータを大幅に削減でき、さらに高い性能を維持できる

### PyTorch関連
- nn.Module
  - PyTorchのニューラルネットワークを作る際の基本クラス
  - 継承時は`super().__init__()`が必須
    - パラメータ管理、デバイス管理、モード切り替えなどがある
- nn.Embedding
  - ルックアップテーブル(参照表)を作成するPyTorchの層
    - 具体的には [50257(語彙) × 768(次元)] の行列など
    - このとき単語ID=1のものは1行目のベクトルが埋め込みとして参照される
- nn.Sequential
  - 複数の層を順番に実行するコンテナ
- nn.Parameter
  - 学習可能なパラメータを作成する
    - torch.ones()のままだと学習時に更新されないが、Parameterでくくることで学習可能に
- nn.Linear
  - 線形変換(全結合層) を行う層
  - y=xW^T + b
  - Linear層は自動的に最後の次元に対して行列乗算する
    - 例えば(2,4,768)@(768,768)だと、(2,4,768)となる
      - @は行列演算(内積)の演算子
      - 最後の次元は(2,4,768)の768
  - 学習可能なパラメータを持つ
- keepdim=True
  - 計算後も元のテンソルと同じ次元数を維持する
- dim=-1
  - 最後の次元を指す。最後の次元に対して計算したい場合などに利用
- view
  - tensorの形状(次元) を変更したい際に、データは変えずに形状だけ変える
    - Wのような大きなtensorを本来の複数のヘッドの状態に戻して、それぞれ独立して計算をし、多ヘッドで多彩な解釈ができるようになる
  - また副次的なメリットとして、GPUの特性上小さな行列の並列計算が得意なのでそういう観点でもメリットがある

# 全体の流れ
- **GPTModel**
  - tokenizerで本の文章(Hello World. I'm ...など) を変換したトークンの単語IDの配列を受け取る
  - 最初にトークン埋め込み層にて、単語IDを実際のembedding(ベクトル形式) に変換する
  - 次に位置埋め込み層にて、位置情報を設定する
    - この層の実態は[0,1,2,3..]のようなベクトル
  - その後、これらの情報を足し合わせxとし、xをドロップアウト層に通す
    - ドロップアウトは過学習を防ぐために使われる
    - その実現方法として、学習時に一部のニューロンをランダムに無効化することで、特定のニューロンらに依存することを避け、汎化性能をより高めている
    - 学習時のみ有効な層で、推論時は利用されない
  - xの形式は [バッチサイズ, トークン数, 埋め込み次元768]
    - 具体的には [2, 4, 768] → 2つの文章、各4トークン、768次元のベクトル
  - その後TransformerBlockにxを渡す。ブロックの数は最初にパラメータで設定しており、今回の場合は12個のブロックがある
    - **TransformerBlock**
    - まず最初にxをshortcutとして一時的に値を保持する。これは後の残差接続で使う
    - 次にレイヤー正規化層に渡す
      - レイヤー正規化層では、データを標準化し、出力の安定化や学習効率の向上を目指す
    - 続いてマルチヘッドアッテンション層に通す
      - **MultiHeadAttention**
        - 入力データxについて
          - xは[バッチサイズ2, トークン数4, 埋め込み次元768]のテンソル
        - 入力xに重み(W_key,W_query,W_value) をかけて、keys, queries, valuesを計算する
        - この時点では、それぞれ複数ヘッド(12個のヘッド*64次元=768)のベクトルがまとめて計算されている
        - 次にviewを用いてテンソルの形状を変える。
          - 具体的には、[バッチサイズ2, トークン数4, ヘッド数12, ヘッド次元64]
        - 続いてtransposeを用いて順序を入れ替える
          - 変換前: [バッチサイズ2, トークン数4, ヘッド数12, ヘッド次元64]
          - 変換後: [バッチサイズ2, ヘッド数12, トークン数4, ヘッド次元64] -> トークン数とヘッド数を入れ替える
        - その後queryとkeysを用いてアテンションスコア計算をする
          - アテンションスコアの計算は、実態は各64次元(ヘッドの次元) のクエリとキーの内積である
          - viewやtransposeは、ここでの内積の計算をそれぞれのヘッドで独立して計算するために行う
        - 算出したスコアに対してmaskをかける
          - これにより未来のトークンが見えなくなる
            - 前提: 
              - 行(query) 現在処理しているトークン
              - 列(key) 注意を向ける対象のトークン
            - 具体例
              - 推論時の実際の流れ：
                - 1. 入力: ["私"]           → 予測: "は"
                - 1. 入力: ["私", "は"]      → 予測: "学生"  
                - 1. 入力: ["私", "は", "学生"] → 予測: "です"
              - マスクしない場合は、「私」を処理する際に、「は」「学生」などの情報が見えてしまう
              - これらの情報が見えた状態で以降の処理でコンテキストベクトルを作成してしまうと、そこにまだ生成されてない単語との関連の情報が埋め込まれてしまう
              - 現在時点で出ている単語だけとの関係性に着目し、まだ出てない単語はmaskで隠してあげることで、過去の情報のみを使って未来を予測できる様にできる
              - 実際の文章推論時も同様に、未来の情報を確認することはできず、過去の文章のみから未来の単語を推論する必要があるため、maskすることは推論時により高精度に推論するための制約である
        - スケーリングとソフトマックスを用いてスコアをアテンション重みとして変換する
          - この重みに対しても特定のニューロンの影響を大きくする可能性を除外するため、ドロップアウトを適用する
        - アテンション重みとvaluesを計算し、重み付き和であるコンテキストベクトルを求める
          - コンテキストベクトルは、元の埋め込みに対してヘッド数分の新たな解釈の情報を付け加えた文脈を含むベクトルである
          - 各トークンの関係の強さを計算するために用いられ、結果としてクエリとキーがどれだけ似ているかのスコアが出る
        - 最後にヘッドが連結していたもとの形に戻しつつ、線形変換を加えてこのベクトルを返却する
          - 最後に線形変換を通すことで、ヘッド間の相互作用を学習し、どのヘッドの譲歩鵜を重視するか調整する
          - もし線形変換しなかった場合は、ただ12このヘッドを連結しただけの独立した情報群になってしまう
    - その後、ドロップアウト層に通す
      - 残差接続の前にドロップアウトすることで、元のx(shortcut)の情報を保護したまま、学習した表現のみにドロップアウトを適用できる
    - 終わり次第、最初にshortcutとして保持しておいた元のxの値を最新のxに加算し、残差接続の実現をする
      - 残差接続を用いることで、勾配消失問題の解決を目指す。深いネットワークでは、誤差逆伝播時にだんだん小さくなって学習が困難になるが、残差接続により勾配が直接的に伝播できるようになる
        - -> 深いネットワークでも安定して学習できるのが嬉しい
    - もう一度xをshortcutとして一時的に値を保持する
    - 次にxをレイヤー正規化層に渡し、その出力をさらにフィードフォワード層にも渡す
      - フィードフォワード層では、データを拡大/縮小変換したり、線形変換を通して、より表現力のある空間の探索を可能にする。
        - もちろんこの層では学習可能なパラメータを持つため、変換の幅は学習時に柔軟に調整することができる
    - その後同じように、ドロップアウト層に通して、最後にshortcutをxに加算して残差接続を行う
  - Transformerブロックから受け取ったデータをレイヤー正規化層に通す
  - 最後に線形変換層にわたし、それぞれの単語IDごとの確率? を出力する
    - 768次元の意味ベクトル -> 50257次元の各単語のスコアに変換
    - embeddingがどの単語にマッチするかのスコアリングのイメージ
    - ロジット: softmax適用前の生の数値スコア

# それぞれのコンポーネント
## GPTModel
- トークン埋め込み層
  - 受け取った単語IDの配列を、埋め込み次元の配列に変換する
- 位置埋め込み層
  - [1024(コンテキストの長さ) × 768(次元)]
  - 最初はランダムな数値が設定される学習可能なパラメータ
- Transformerブロック

## TransformerBlock


## FeedForward
- フィードフォワード
  - 各位置の特徴を独立に拡大/縮小変換し、さらに非線形変換を通してより表現力のある空間の探索を可能にする
  - 具体的には、まず高次元の空間に拡張(拡大変換)し、それをGELUで非線形変換、その後元の次元に戻す(縮小変換)
  - データから学習して汎化するモデルの能力を向上させるうえで、重要な役割を果たす
  - なぜ4倍? -> 過去の研究により、4倍にするのが経験的に良い結果を示している

## LayerNorm
- Layer Normalization, 層正規化
- コンセプトはNN層の出力を平均0,分散1になるよう調整すること
  - これにより、効果的な重みへの収束が早まり、訓練の一貫性と信頼性を確保する
- ただ標準化するだけでなく、学習可能なパラメータであるscaleとshiftを利用し、より柔軟に標準化を実施する 
  - 具体的には特定の次元(768のうちの)の影響が大きいと学習時に発覚した場合に、重要度の表現の仕方としてscaleの値を大きく設定するなど

## MultiHeadAttention

- ヘッドとは
  - 異なる注意の向け方を学習する機構
    - 例えば、文法の関係に注目したヘッドや、意味的な関係に注目したヘッド、修飾関係に注目したヘッドなど
- マルチヘッドとは
  - ヘッドが複数あること(マルチ)
    - 複数あることで上述したような様々な観点から注意を向けることで文章に対する理解がより深まる
  - 1つの注意機構のヘッドでは、1つの関係性しか捉えられないが、複雑な文章では同時に複数の関係性が重複している
  - 理論的には複数のヘッドを用いてそれぞれ独立して計算をするが、
  - 実装面でいうとヘッドのベクトルをすべて直列に結合し、大きなベクトルとしてまとめて計算を行っている
  - 理論をそのまま実装はせず、計算効率のためにうまく工夫されている


## その他疑問に対するメモ
- なぜ複数箇所でドロップアウトをしているのか
  - 複数箇所
    - drop_emb(GPTModel): 埋め込み層の後
    - dropout(MultiHeadAttention): アテンションの重みに対して
    - drop_shortcut(TransformerBlock): 各サブ層の出力に対して
  - なぜか -> それぞれ目的が違う
    - 埋め込み層: 入力の過学習を防ぐ
    - アテンション層: 注意重みの過学習を防ぐ
    - TransformerBlock: 各サブ層の出力の過学習を防ぐ